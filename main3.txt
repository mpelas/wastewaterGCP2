import functions_framework
import requests
import json
import hashlib
from shapely.geometry import Point, mapping, shape
from shapely.ops import cascaded_union, transform  # Add `transform` import
import math
from google.cloud import storage
from pyproj import CRS, Transformer  # Add for reprojection

# Constants for the Cloud Function.
WASTEWATER_API_URL = "https://astikalimata.ypeka.gr/api/query/wastewatertreatmentplants"
GCS_BUCKET_NAME = "mpelas-wastewater-bucket"
PERIFEREIES_GEOJSON_PATH = "perifereiesWGS84.geojson"
LAST_HASH_FILE_PATH = "wastewater_data_hash.txt"
OUTPUT_GEOJSON_PATH = "no_swim_zones/wastewater_no_swim_zones.geojson"
BUFFER_DISTANCE_METERS = 200

# Define coordinate reference systems
WGS84_CRS = CRS("EPSG:4326")  # Standard GPS coordinates
GREEK_GRID_CRS = CRS("EPSG:2100")  # Greek Grid for accurate meters

# Create transformers
transformer_to_greek_grid = Transformer.from_crs(WGS84_CRS, GREEK_GRID_CRS, always_xy=True)
transformer_to_wgs84 = Transformer.from_crs(GREEK_GRID_CRS, WGS84_CRS, always_xy=True)

def get_gcs_blob(bucket_name, blob_name):
    """Retrieves a blob from Google Cloud Storage."""
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    return bucket.blob(blob_name)

def load_perifereies_data(bucket_name, file_path):
    """Loads and parses the large perifereies GeoJSON file from GCS."""
    try:
        blob = get_gcs_blob(bucket_name, file_path)
        geojson_data = blob.download_as_text()
        perifereies_features = json.loads(geojson_data)
        perifereies_geometries = [
            shape(f['geometry']) for f in perifereies_features['features']
        ]
        print("====DIABASA tis perifereies perifereiesWGS84.geojson")
        print(perifereies_geometries)
        return perifereies_geometries
    except Exception as e:
        print(f"Error loading perifereies GeoJSON: {e}")
        return None

def calculate_new_zones(perifereies_geometries, wastewater_data):
    """
    Performs the core geospatial analysis: buffering, union, and difference.
    """
    print("Starting geospatial analysis...")
    all_buffers = []

    # Check if the data is a FeatureCollection or a list of features
    if isinstance(wastewater_data, dict) and 'features' in wastewater_data:
        features_to_process = wastewater_data['features']
    elif isinstance(wastewater_data, list):
        features_to_process = wastewater_data
    else:
        print("Invalid wastewater data format.")
        return None

    print("XXXXXXXXXXXXX features_to_process=")
    print(features_to_process)
    
    for plant_feature in features_to_process:
        try:
            # Check for standard GeoJSON 'properties' key first
            if 'properties' in plant_feature:
                props = plant_feature['properties']
            else:
                # If 'properties' key is not present, assume the top-level keys are the properties
                props = plant_feature.get('properties', plant_feature)

            # Prepare metadata for the final GeoJSON feature
            # Only include relevant metadata fields
            metadata = {
                'code': props.get('code'),
                'name': props.get('name'),
                'receiverName': props.get('receiverName'),
                'receiverNameEn': props.get('receiverNameEn'),
                'receiverWaterType': props.get('receiverWaterType'),
                'latitude': props.get('latitude'),
                'longitude': props.get('longitude')
            }



            receiver_location_wkt = props.get('receiverLocation')
            longitude = props.get('longitude')
            latitude = props.get('latitude')

            point = None
            if receiver_location_wkt:
                try:
                    point = wkt.loads(receiver_location_wkt)
                except Exception as e:
                    print(f"Error parsing WKT for plant '{props.get('name')}': {e}. Falling back to main coordinates.")
            
            if point is None:
                if longitude is not None and latitude is not None:
                    point = Point(longitude, latitude)
                else:
                    print(f"Skipping plant '{props.get('name')}' due to missing coordinates.")
                    continue

            print("name=", props.get('name'))
            print("longitude=", point.x)
            print("latitude=", point.y)
            
            # Calculate buffer
            # Convert 200m buffer distance to degrees at the given latitude
            buffer_lat_deg, buffer_lon_deg = meters_to_degrees(BUFFER_DISTANCE_METERS, math.radians(point.y))
            buffer_radius = buffer_lat_deg # Use latitude degree equivalent as the average radius

            buffered_point = point.buffer(buffer_radius)
            
            # Perform Difference: Find the part of the buffer that is *not* on the mainland
            danger_zone = buffered_point.difference(unified_perifereies)
            
            # If the difference results in a valid geometry (i.e., not empty), save it
            if not danger_zone.is_empty:
                no_swim_zones_with_metadata.append((danger_zone, metadata))
     
        except Exception as e:
            print(f"Skipping plant due to an error processing its data: {e}")
            continue

    print(f"Geospatial analysis complete. Found {len(no_swim_zones_with_metadata)} no-swim zones.")
    return no_swim_zones_with_metadata

# --- EXECUTION FLOW MODIFIED HERE ---
@functions_framework.http
def check_for_changes(request):
    """
    Main entry point for the Google Cloud Function.
    Fetches wastewater data, checks for changes, and if found,
    recalculates and updates the no-swimming zones with metadata.
    """
    print("Function started.")
    # 1. Fetch wastewater data
    try:
        response = requests.get(WASTEWATER_API_URL, timeout=30)
        response.raise_for_status()
        wastewater_data = response.json()
        current_data_string = json.dumps(wastewater_data, sort_keys=True)
        current_hash = hashlib.sha256(current_data_string.encode('utf-8')).hexdigest()
    except requests.exceptions.RequestException as e:
        print(f"Failed to fetch data from API: {e}")
        return ("Failed to fetch data.", 500)
    # 2. Compare hash
    try:
        hash_blob = get_gcs_blob(GCS_BUCKET_NAME, LAST_HASH_FILE_PATH)
        if hash_blob.exists():
            last_hash = hash_blob.download_as_text()
            if current_hash == last_hash:
                print("No changes detected. Exiting.")
                return ("No changes detected.", 200)
        else:
            print("No previous hash found. Proceeding with analysis.")
    except Exception as e:
        print(f"Error checking last hash: {e}")
    # 3. Load perifereies data
    perifereies_geometries = load_perifereies_data(GCS_BUCKET_NAME, PERIFEREIES_GEOJSON_PATH)
    if perifereies_geometries is None:
        return ("Failed to load perifereies data.", 500)
    # 4. Calculate danger zones
    danger_zones_geometry = calculate_new_zones(perifereies_geometries, wastewater_data)
    if danger_zones_geometry is None:
        return ("Analysis failed.", 500)
    # 5. Save results
    try:
        new_zones_geojson = {
            "type": "FeatureCollection",
            "features": [
                {
                    "type": "Feature",
                    "geometry": mapping(danger_zones_geometry),
                    "properties": {}
                }
            ]
        }
        output_blob = get_gcs_blob(GCS_BUCKET_NAME, OUTPUT_GEOJSON_PATH)
        output_blob.upload_from_string(
            json.dumps(new_zones_geojson),
            content_type="application/geo+json"
        )
        print(f"Saved to GCS: gs://{GCS_BUCKET_NAME}/{OUTPUT_GEOJSON_PATH}")
        # Update hash
        hash_blob = get_gcs_blob(GCS_BUCKET_NAME, LAST_HASH_FILE_PATH)
        hash_blob.upload_from_string(current_hash)
        print("Hash file updated.")
    except Exception as e:
        print(f"Failed to save results: {e}")
        return ("Failed to save results.", 500)
    return ("Analysis complete. New zones saved.", 200)
